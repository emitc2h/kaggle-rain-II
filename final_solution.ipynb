{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Pandas\n",
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "pd.set_option('display.mpl_style', 'default') \n",
    "pd.set_option('display.width', 5000) \n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "## Numpy & Scikit-learn\n",
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "\n",
    "## other python imports\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load training and testing datasets\n",
    "## Obtain them from https://www.kaggle.com/c/how-much-did-it-rain-ii/data\n",
    "## and put them in the same directory as this jupyter notebook\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df  = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Split the data into categories\n",
    "My approach is to use boosted regression trees to predict the rain gauge measurements from the radar data. Boosted regression trees do best with a fair amount of thought going into feature engineering. The data is thus organized in 5 categories, which all differ in the availability of the features provided by the dataset. Having specialized boosted trees treat each category using only the features that are available is a feature engineering strategy as well as a data cleaning strategy. The categories are:\n",
    "\n",
    "   * **Polarized:** No missing values for all features, for all entries in the time series.\n",
    "   * **Basic:** No missing values for reflectivity features only, polarization features consistently missing.\n",
    "   * **Null:** All values are missing for all entries in the time series.\n",
    "   * **Partial Polarized:** At least some values missing in both reflectivity and polarization features.\n",
    "   * **Partial Basic:** All polarized features are missing, at least some values missing in reflectivity features.\n",
    "   \n",
    "Entries where main reflectivity values are missing are ignored in the evaluation of the leaderboard score.\n",
    "\n",
    "A Splitter class is written to split the training and testing datasets consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define convenient groups of columns\n",
    "all_columns = [\n",
    "    'Ref',\n",
    "    'Ref_5x5_10th',\n",
    "    'Ref_5x5_50th',\n",
    "    'Ref_5x5_90th',\n",
    "    'RefComposite',\n",
    "    'RefComposite_5x5_10th',\n",
    "    'RefComposite_5x5_50th',\n",
    "    'RefComposite_5x5_90th',\n",
    "    'RhoHV',\n",
    "    'RhoHV_5x5_10th',\n",
    "    'RhoHV_5x5_50th',\n",
    "    'RhoHV_5x5_90th',\n",
    "    'Zdr',\n",
    "    'Zdr_5x5_10th',\n",
    "    'Zdr_5x5_50th',\n",
    "    'Zdr_5x5_90th',\n",
    "    'Kdp',\n",
    "    'Kdp_5x5_10th',\n",
    "    'Kdp_5x5_50th',\n",
    "    'Kdp_5x5_90th'\n",
    "]\n",
    "\n",
    "basic_columns = all_columns[:8]\n",
    "polarized_columns = all_columns[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ========================================\n",
    "class Splitter:\n",
    "    \"\"\"\n",
    "    A tool class to split both training and testing datasets into the 5 categories\n",
    "    according to the exact same rules\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ## ------------------------------------------\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "\n",
    "        self.df = dataframe\n",
    "\n",
    "        ## Transform the dataset into booleans, indicating True for present values\n",
    "        ## and false for missing values\n",
    "        self.bool_df = self.df.notnull()\n",
    "        self.bool_df['Id'] = self.df['Id']\n",
    "\n",
    "        ## Collapse the time series in two different ways:\n",
    "        ##     all: all values in time are present\n",
    "        ##     any: any one value in time is present\n",
    "        ##\n",
    "        ## from these we can derive:\n",
    "        ##     not any: no value is present\n",
    "\n",
    "        all_df = self.bool_df.groupby('Id').agg(np.all)\n",
    "        any_df = self.bool_df.groupby('Id').agg(np.any)\n",
    "\n",
    "        self.ref_mask           = any_df['Ref']\n",
    "\n",
    "        self.basic_all_mask     = all_df[basic_columns].all(axis=1)\n",
    "        self.polarized_all_mask = all_df[polarized_columns].all(axis=1)\n",
    "\n",
    "        self.basic_null_mask     = ~any_df[basic_columns].any(axis=1)\n",
    "        self.polarized_null_mask = ~any_df[polarized_columns].any(axis=1)\n",
    "\n",
    "        self.basic_partial_mask     = ~self.basic_null_mask & ~self.basic_all_mask\n",
    "        self.polarized_partial_mask = ~self.polarized_null_mask & ~self.polarized_all_mask\n",
    "\n",
    "\n",
    "    ## -------------------------------------------\n",
    "    def polarized(self):\n",
    "        \"\"\"\n",
    "        Guarantees complete information\n",
    "        \"\"\"\n",
    "\n",
    "        return self.df[self.df['Id'].map(self.basic_all_mask & self.polarized_all_mask)]\n",
    "\n",
    "\n",
    "    ## -------------------------------------------\n",
    "    def basic(self):\n",
    "        \"\"\"\n",
    "        Guarantees complete information for the reflectivity, but incomplete or\n",
    "        absent information for the polarization values\n",
    "        \"\"\"\n",
    "\n",
    "        return self.df[self.df['Id'].map(self.basic_all_mask & self.polarized_null_mask)]\n",
    "\n",
    "\n",
    "    ## -------------------------------------------\n",
    "    def null(self):\n",
    "        \"\"\"\n",
    "        Gurantees that no information is present\n",
    "        \"\"\"\n",
    "\n",
    "        return self.df[self.df['Id'].map(self.basic_null_mask | ~self.ref_mask)]\n",
    "\n",
    "\n",
    "    ## --------------------------------------------\n",
    "    def partial_basic(self):\n",
    "        \"\"\"\n",
    "        Gurantees that only a fraction of the reflectivity data is available\n",
    "        \"\"\"\n",
    "\n",
    "        return self.df[self.df['Id'].map(self.ref_mask & self.basic_partial_mask & self.polarized_null_mask)]\n",
    "\n",
    "\n",
    "    ## --------------------------------------------\n",
    "    def partial_polarized(self):\n",
    "        \"\"\"\n",
    "        Guarantees that at least some basic information is present,\n",
    "        and that only a partial set of polarization information is present\n",
    "        \"\"\"\n",
    "\n",
    "        cross = self.basic_partial_mask | self.polarized_partial_mask\n",
    "        none  = self.basic_null_mask | self.polarized_null_mask\n",
    "\n",
    "        return self.df[self.df['Id'].map(self.ref_mask & cross & ~none)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Pass the datasets to the splitters\n",
    "train_splt = Splitter(train_df)\n",
    "test_splt  = Splitter(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Obtain the split datasets\n",
    "polarized_df         = train_splt.polarized()\n",
    "basic_df             = train_splt.basic()\n",
    "null_df              = train_splt.null()\n",
    "partial_basic_df     = train_splt.partial_basic()\n",
    "partial_polarized_df = train_splt.partial_polarized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Ensure the resulting groups are both inclusive and exclusive\n",
    "n_complete = len(train_df)\n",
    "\n",
    "n_polarized = len(polarized_df)\n",
    "print 'Polarized DF               :', n_polarized, '({:.2%})'.format(float(n_polarized)/n_complete)\n",
    "\n",
    "n_basic = len(basic_df)\n",
    "print 'Basic DF                   :', n_basic, '({:.2%})'.format(float(n_basic)/n_complete)\n",
    "\n",
    "n_null = len(null_df)\n",
    "print 'Null DF                    :', n_null, '({:.2%})'.format(float(n_null)/n_complete)\n",
    "\n",
    "n_partial_basic = len(partial_basic_df)\n",
    "print 'Partial Basic DF           :', n_partial_basic, '({:.2%})'.format(float(n_partial_basic)/n_complete)\n",
    "\n",
    "n_partial_polarized = len(partial_polarized_df)\n",
    "print 'Partial Polarized DF       :', n_partial_polarized, '({:.2%})'.format(float(n_partial_polarized)/n_complete)\n",
    "\n",
    "print '-'*47\n",
    "n_all = n_polarized + n_basic + n_null + n_partial_basic + n_partial_polarized\n",
    "print 'Putting them back together :', n_all, '({:.2%})'.format(float(n_all)/n_complete)\n",
    "\n",
    "print 'Complete DF                :', n_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data cleaning and time-series integration\n",
    "There are 5 features provided in this dataset, and for each feature, the value in the air column right above the rain gauge is what I will call the main value. The 10th, 50th and 90th quantile of the distribution of values in a 5x5 grid around the rain gauge are also provided, and I use them to do a very simple linear extrapolation and guess the main value when it is missing.\n",
    "\n",
    "The reflectivity is related to the rate of precipitation. Therefore, in order to obtain a cumulative amount, the reflectivity should be integrated over time. The resulting number is hopefully proportional to the total amount of rain on the ground during the hour. Since the number of entries in each time series vary, the total number of features usable for each rain gauge prediction varies as well. This is innapropriate for boosted regression trees, as a constant number of input features is expected. The time series are then boiled down to single numbers. The reflectivities are integrated, and the polarimetric features are averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Calculate extrapolation factors from distributions to main values for the reflectivities and RhoHV\n",
    "## Simply calculate the ratio of the means using the polarized DF\n",
    "\n",
    "Ref   = np.nanmean(polarized_df['Ref'].values)\n",
    "Ref10 = np.nanmean(polarized_df['Ref_5x5_10th'].values)\n",
    "Ref50 = np.nanmean(polarized_df['Ref_5x5_50th'].values)\n",
    "Ref90 = np.nanmean(polarized_df['Ref_5x5_90th'].values)\n",
    "\n",
    "RefComposite   = np.nanmean(polarized_df['RefComposite'].values)\n",
    "RefComposite10 = np.nanmean(polarized_df['RefComposite_5x5_10th'].values)\n",
    "RefComposite50 = np.nanmean(polarized_df['RefComposite_5x5_50th'].values)\n",
    "RefComposite90 = np.nanmean(polarized_df['RefComposite_5x5_90th'].values)\n",
    "\n",
    "RhoHV   = np.nanmean(polarized_df['RhoHV'].values)\n",
    "RhoHV10 = np.nanmean(polarized_df['RhoHV_5x5_10th'].values)\n",
    "RhoHV50 = np.nanmean(polarized_df['RhoHV_5x5_50th'].values)\n",
    "RhoHV90 = np.nanmean(polarized_df['RhoHV_5x5_90th'].values)\n",
    "\n",
    "cf = {\n",
    "    'Ref'          : (Ref/Ref50,                   Ref/Ref10,                   Ref/Ref90),\n",
    "    'RefComposite' : (RefComposite/RefComposite50, RefComposite/RefComposite10, RefComposite/RefComposite90),\n",
    "    'RhoHV'        : (RhoHV/RhoHV50,               RhoHV/RhoHV10,               RhoHV/RhoHV90)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## -------------------------------------------\n",
    "def complete_main_values(df, columns=[]):\n",
    "    \"\"\"\n",
    "    Completes the main variables using the distributions\n",
    "    \"\"\"\n",
    "    \n",
    "    for var in columns:\n",
    "        df[var] = df[var].fillna(value=cf[var][0]*df['{0}_5x5_50th'.format(var)])\n",
    "        df[var] = df[var].fillna(value=cf[var][1]*df['{0}_5x5_10th'.format(var)])\n",
    "        df[var] = df[var].fillna(value=cf[var][2]*df['{0}_5x5_90th'.format(var)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define a function to assign a time interval to each time series entry\n",
    "## which will be used to do a coarse rectangular integration\n",
    "\n",
    "## -------------------------------------------\n",
    "def intervals(df):\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe assigning a time interval for each entry\n",
    "    \"\"\"\n",
    "    \n",
    "    df['interval'] = df['minutes_past'].diff()\n",
    "    \n",
    "    ## Build two masks, to single out first and last rows of each group\n",
    "    group_begin_mask = df['Id'].diff() != 0\n",
    "    group_end_mask   = df['Id'].diff().shift(-1) != 0\n",
    "    \n",
    "    ## Patch the beginning entry of each group\n",
    "    df['interval'][group_begin_mask] = df['minutes_past'][group_begin_mask]\n",
    "    \n",
    "    ## Patch the last entry of each group\n",
    "    df['interval'][group_end_mask] = 60 + df['interval'][group_end_mask] - df['minutes_past'][group_end_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Write a class to collapse the time series according to given instructions\n",
    "## The option to super_integrate replace the main values by an average value\n",
    "## of the main value, the 10th, the 50th and th 90th quantiles. It seems to\n",
    "## make the predictions a little more accurate in some categories. One possible\n",
    "## reason for this is the motion of the clouds. A single radar reading is a\n",
    "## snapshot in time, but between the readings, the clouds move. Between two\n",
    "## separate readings, the clouds above the rain gauge are not going to be\n",
    "## exactly the same as at the time of the readings. It might matter to take the\n",
    "## 5x5 distributions to account for cloud motion, but it can't be very precise.\n",
    "## This is why I don't bother doing more than this very crude accounting of\n",
    "## the 5x5 distributions. Ideally, You would have the actual readings in the 5x5\n",
    "## grid, and you could interpolate what cloud region was above the rain gauge\n",
    "## at any instant. The quantiles simply do not hold the information to do this.\n",
    "\n",
    "## ========================================\n",
    "class Collapser:\n",
    "    \"\"\"\n",
    "    A class to collapse the time series\n",
    "    \"\"\"\n",
    "\n",
    "    ## -------------------------------------------\n",
    "    def __init__(self, integrate=[], mean=all_columns, patch=[], super_integrate=False):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "\n",
    "        self.integrate = integrate\n",
    "        self.super_integrate = super_integrate\n",
    "        self.mean = list(set(mean).difference(set(integrate)))\n",
    "        self.patch = patch\n",
    "\n",
    "\n",
    "    ## -------------------------------------------\n",
    "    def collapse(self, df):\n",
    "        \"\"\"\n",
    "        Collapse the time series for a dataset given the rules\n",
    "        specified in the constructor\n",
    "        \"\"\"\n",
    "\n",
    "        intervals(df)\n",
    "        complete_main_values(df, self.patch)\n",
    "\n",
    "        int_columns = []\n",
    "        \n",
    "        ## Multiply the quantities by the time intervals\n",
    "        for var in self.integrate:\n",
    "            name = 'Int_{0}'.format(var)\n",
    "            df[name] = df['interval'] * df[var]\n",
    "            int_columns.append(name)\n",
    "            \n",
    "        ## Different values need to be grouped in different ways\n",
    "        try:\n",
    "            expected  = df[['Id', 'Expected', 'radardist_km'] + self.mean]\n",
    "        except KeyError:\n",
    "            expected  = df[['Id', 'radardist_km'] + self.mean]\n",
    "            \n",
    "        expected  = expected.groupby('Id', as_index=False).mean()\n",
    "    \n",
    "        variables = df[['Id'] + int_columns]\n",
    "        variables = variables.groupby('Id', as_index=False).sum()\n",
    "        \n",
    "        ## Fold the 5x5 distributions into the integration\n",
    "        if self.super_integrate:\n",
    "            if 'Ref' in self.integrate:\n",
    "                variables['Int_Ref'] = variables['Int_Ref'] + \\\n",
    "                variables['Int_Ref_5x5_10th'] + \\\n",
    "                variables['Int_Ref_5x5_50th'] + \\\n",
    "                variables['Int_Ref_5x5_90th']\n",
    "                \n",
    "            if 'RefComposite' in self.integrate:\n",
    "                variables['Int_RefComposite'] = variables['Int_Ref'] + \\\n",
    "                variables['Int_RefComposite_5x5_10th'] + \\\n",
    "                variables['Int_RefComposite_5x5_50th'] + \\\n",
    "                variables['Int_RefComposite_5x5_90th']\n",
    "                \n",
    "            if 'RhoHV' in self.integrate:\n",
    "                variables['Int_RhoHV'] = variables['Int_Ref'] + \\\n",
    "                variables['Int_RhoHV_5x5_10th'] + \\\n",
    "                variables['Int_RhoHV_5x5_50th'] + \\\n",
    "                variables['Int_RhoHV_5x5_90th']\n",
    "                \n",
    "            if 'Zdr' in self.integrate:\n",
    "                variables['Int_Zdr'] = variables['Int_Ref'] + \\\n",
    "                variables['Int_Zdr_5x5_10th'] + \\\n",
    "                variables['Int_Zdr_5x5_50th'] + \\\n",
    "                variables['Int_Zdr_5x5_90th']\n",
    "                \n",
    "            if 'Kdp' in self.integrate:\n",
    "                variables['Int_Kdp'] = variables['Int_Ref'] + \\\n",
    "                variables['Int_Kdp_5x5_10th'] + \\\n",
    "                variables['Int_Kdp_5x5_50th'] + \\\n",
    "                variables['Int_Kdp_5x5_90th']\n",
    "        \n",
    "        ## transform the integrated values into average rates\n",
    "        for var in int_columns:\n",
    "            if self.super_integrate:\n",
    "                variables[var] = variables[var]/240.0\n",
    "            else:\n",
    "                variables[var] = variables[var]/60.0\n",
    "            \n",
    "        ## Rename columns to what they are supposed to be\n",
    "        renamer = {}\n",
    "        for original, int_name in zip(self.integrate, int_columns):\n",
    "            renamer[int_name] = original\n",
    "            \n",
    "        variables.rename(columns=renamer, inplace=True)\n",
    "            \n",
    "        ## Merge back expected and variables\n",
    "        return pd.merge(variables, expected, on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Run the collapsers on the training datasets\n",
    "\n",
    "## Polarized DF : integrate the reflectivities, average the polarimetric features\n",
    "polarized_collapser         = Collapser(integrate=basic_columns, super_integrate=True)\n",
    "polarized_collapsed_df      = polarized_collapser.collapse(polarized_df)\n",
    "test_polarized_collapsed_df = polarized_collapser.collapse(test_splt.polarized())\n",
    "\n",
    "## Basic DF : integrate the reflectivities\n",
    "basic_collapser         = Collapser(integrate=basic_columns, super_integrate=True)\n",
    "basic_collapsed_df      = basic_collapser.collapse(basic_df)\n",
    "test_basic_collapsed_df = basic_collapser.collapse(test_splt.basic())\n",
    "\n",
    "## Null DF : doesn't really matter, just collapse the time series\n",
    "null_collapser         = Collapser()\n",
    "null_collapsed_df      = null_collapser.collapse(null_df)\n",
    "test_null_collapsed_df = null_collapser.collapse(test_splt.null())\n",
    "\n",
    "## Partial Polarized DF : patch the reflectivities, average the polarimetric features\n",
    "partial_polarized_collapser         = Collapser(patch=['Ref', 'RefComposite'])\n",
    "partial_polarized_collapsed_df      = partial_polarized_collapser.collapse(partial_polarized_df)\n",
    "test_partial_polarized_collapsed_df = partial_polarized_collapser.collapse(test_splt.partial_polarized())\n",
    "\n",
    "## Partial Basic DF : Only patch the composite Reflectivity, average everything else\n",
    "partial_basic_collapser         = Collapser(patch=['RefComposite'])\n",
    "partial_basic_collapsed_df      = partial_basic_collapser.collapse(partial_basic_df)\n",
    "test_partial_basic_collapsed_df = partial_basic_collapser.collapse(test_splt.partial_basic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Write a function to clean repeat rain gauge values. Rain gauges can be stuck and result\n",
    "## in erroneous data. These readings should be ignored while training the decision trees. Since\n",
    "## these cases while also occur in the testing datasets, they will artificially drag down\n",
    "## the score.\n",
    "\n",
    "## ---------------------------------------\n",
    "def clean_stuck_gauges(dataframe, threshold=50):\n",
    "    \"\"\"\n",
    "    Remove rain gauge values that occur too often, indicating that those gauges are stuck\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe['count'] = 1\n",
    "    counts = dataframe.groupby('Expected', as_index=False).count()\n",
    "    large_counts = counts[counts['count'] > threshold]\n",
    "\n",
    "    def high_count(expected):\n",
    "        return len(large_counts[large_counts['Expected'] == expected]) > 0\n",
    "\n",
    "    dataframe['large_count'] = map(high_count, dataframe['Expected'])\n",
    "    clean_dataframe = dataframe[dataframe['large_count']]\n",
    "\n",
    "    clean_dataframe.drop('count', 1)\n",
    "    clean_dataframe.drop('large_count', 1)\n",
    "\n",
    "    return clean_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Boosted regression tree training and prediction median adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Polarized DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Split the training dataset in training and testing datasets for crude BRT optimization, but also to check the effect\n",
    "## of doing additional data cleaning on the training dataset on the BRT performance\n",
    "\n",
    "split = len(polarized_collapsed_df)/2\n",
    "train = polarized_collapsed_df[:split]\n",
    "test  = polarized_collapsed_df[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Write a class to train, store and apply a boosted regression tree\n",
    "## The training dataset requires additional cleaning such as\n",
    "##     1. Making sure no NaN values are left, replace with dummy value of -1111.0\n",
    "##     2. Remove outliers. A meter of rain on the ground during 1h doesn't make much sense\n",
    "##     3. Clean stuck gauges. If a specific value for a rain gauge prediction occurs too\n",
    "##        often, remove all entries with that value from the training dataset\n",
    "## ========================================\n",
    "\n",
    "class BRTTrainer:\n",
    "    \"\"\"\n",
    "    A class to train a Boosted Regression Trees\n",
    "    \"\"\"\n",
    "\n",
    "    ## --------------------------------------------\n",
    "    def __init__(self, train_df, variables=all_columns, max_depth=None, n_estimators=10, max_features=None):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.brt = None\n",
    "        self.variables = variables\n",
    "        self.max_depth = max_depth\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "\n",
    "\n",
    "    ## --------------------------------------------\n",
    "    def train(self, clean=False, stuck_repeat=50, max_expected=100.0):\n",
    "        \"\"\"\n",
    "        Train the BRT\n",
    "        \"\"\"\n",
    "\n",
    "        ## Do some additional cleaning only on the training datasets\n",
    "        clean_df = self.train_df.fillna(-1111.0)\n",
    "        if clean:\n",
    "            clean_df = clean_stuck_gauges(clean_df, threshold=stuck_repeat)\n",
    "            clean_df = clean_df[clean_df['Expected'] < max_expected]\n",
    "\n",
    "        ## Extract numpy arrays to pass to scikit-learn for training\n",
    "        train_data   = clean_df[['radardist_km'] + self.variables].values\n",
    "        train_target = clean_df['Expected'].values\n",
    "\n",
    "        ## Train the Boosted Regression trees\n",
    "        self.brt = ensemble.GradientBoostingRegressor(\n",
    "            max_depth=self.max_depth,\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_features=self.max_features,\n",
    "            random_state=24)\n",
    "        \n",
    "        self.brt.fit(train_data, train_target)\n",
    "\n",
    "\n",
    "    ## ---------------------------------------------\n",
    "    def predict(self, *args):\n",
    "        \"\"\"\n",
    "        Produce prediction for a single entry\n",
    "        \"\"\"\n",
    "\n",
    "        return self.brt.predict(args)[0]\n",
    "\n",
    "\n",
    "    ## ---------------------------------------------\n",
    "    def apply(self, df):\n",
    "        \"\"\"\n",
    "        Create a new prediction column for a dataset\n",
    "        \"\"\"\n",
    "\n",
    "        clean_df = df.fillna(-1111.0)\n",
    "        \n",
    "        columns = [clean_df['radardist_km']]\n",
    "        for var in self.variables:\n",
    "            columns.append(clean_df[var])\n",
    "\n",
    "        df['Prediction'] = map(self.predict, *columns)\n",
    "        \n",
    "        \n",
    "    ## ---------------------------------------------\n",
    "    def evaluate(self, df):\n",
    "        \"\"\"\n",
    "        Evaluate the performance of the predictions\n",
    "        This is the same metric used for the leaderboard\n",
    "        \"\"\"\n",
    "        \n",
    "        return metrics.mean_absolute_error(df['Expected'].values, df['Prediction'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train a BRT on half the training dataset. Parameters have been roughly optimized by hand\n",
    "\n",
    "polarized_trainer = BRTTrainer(train, variables=all_columns, max_depth=3, n_estimators=200)\n",
    "polarized_trainer.train(clean=True, stuck_repeat=10, max_expected=20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Evaluate on the second half of the training dataset\n",
    "\n",
    "polarized_trainer.apply(test)\n",
    "polarized_trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create a class to make scatter plot matrices\n",
    "\n",
    "## ========================================\n",
    "class ScatterMatrix:\n",
    "    \"\"\"\n",
    "    A class to create a matrix of scatter plots from a pandas data frame\n",
    "    \"\"\"\n",
    "    \n",
    "    ## ----------------------------------------\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = dataframe\n",
    "        \n",
    "        self.n = 0\n",
    "        self.variables = []\n",
    "        self.labels = []\n",
    "        self.lo = []\n",
    "        self.hi = []\n",
    "        \n",
    "        \n",
    "    ## ----------------------------------------\n",
    "    def add(self, variable_name, lo=None, hi=None, label=None):\n",
    "        \"\"\"\n",
    "        Add a variable to the matrix:\n",
    "        'variable_name' is the name of the variable to plot as it appears in the data frame\n",
    "        'label' is how the variable name should appear on the plot. If left unspecified, 'variable_name' will be used.\n",
    "        'lo' is the lower bound to show for this variable. If left unspecifed, the minimum value will be used\n",
    "        'hi' is the highest bound...\n",
    "        \"\"\"\n",
    "        \n",
    "        self.variables.append(variable_name)\n",
    "        \n",
    "        if label is None:\n",
    "            label = variable_name\n",
    "        self.labels.append(label)\n",
    "        \n",
    "        if lo is None:\n",
    "            lo = self.data[variable_name].values.min()\n",
    "        self.lo.append(lo)\n",
    "        \n",
    "        if hi is None:\n",
    "            hi = self.data[variable_name].values.max()\n",
    "        self.hi.append(hi)\n",
    "        \n",
    "        self.n += 1\n",
    "        \n",
    "        \n",
    "    ## ----------------------------------------\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        produces the plot\n",
    "        \"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=self.n, ncols=self.n, figsize=(2*self.n,2*self.n))\n",
    "        fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "        \n",
    "        for i in range(self.n):\n",
    "            for j in range(self.n):\n",
    "                \n",
    "                ## Put column j on the x-axis and column i on the y-axis\n",
    "                if i != j: axes[i][j].hist2d(\n",
    "                                        self.data[self.variables[j]].values, \n",
    "                                        self.data[self.variables[i]].values, \n",
    "                                        cmap=plt.cm.YlOrRd_r,\n",
    "                                        bins=[40,40],\n",
    "                                        range=[[self.lo[j], self.hi[j]], [self.lo[i], self.hi[i]]]\n",
    "                                        )\n",
    "                    \n",
    "                ## If i==j, display the feature name \n",
    "                else:\n",
    "                    \n",
    "                    axes[i][j].set_axis_bgcolor('white')\n",
    "                    for spine in axes[i][j].spines.values():\n",
    "                        spine.set_edgecolor('white')\n",
    "                    \n",
    "                    axes[i][j].annotate(\n",
    "                                self.labels[i],\n",
    "                                (0.5, 0.5),\n",
    "                                xycoords='axes fraction',\n",
    "                                ha='center',\n",
    "                                va='center'\n",
    "                                )\n",
    "                    \n",
    "                ## Hide axis labels strategically\n",
    "                axes[i][j].xaxis.set_visible(False)\n",
    "                axes[i][j].yaxis.set_visible(False)\n",
    "                axes[i][j].grid(False)\n",
    "                \n",
    "                if (i == self.n -1) and (j%2 == 0):\n",
    "                    axes[i][j].xaxis.set_ticks_position('bottom')\n",
    "                    axes[i][j].xaxis.set_visible(True)\n",
    "                    \n",
    "                if (i == 0) and (j%2-1 == 0):\n",
    "                    axes[i][j].xaxis.set_ticks_position('top')\n",
    "                    axes[i][j].xaxis.set_visible(True)\n",
    "                    \n",
    "                if (j == self.n -1) and (i%2 == 0):\n",
    "                    axes[i][j].yaxis.set_ticks_position('right')\n",
    "                    axes[i][j].yaxis.set_visible(True)\n",
    "                    \n",
    "                if (j == 0) and (i%2-1 == 0):\n",
    "                    axes[i][j].yaxis.set_ticks_position('left')\n",
    "                    axes[i][j].yaxis.set_visible(True)\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make a scatter plot matrix for the Polarized DF\n",
    "sm = ScatterMatrix(test)\n",
    "sm.add('radardist_km', 0, 20)\n",
    "sm.add('Ref', 0, 50)\n",
    "sm.add('RefComposite', 0, 100)\n",
    "sm.add('RhoHV', 0.8, 1.05)\n",
    "sm.add('Zdr', -2,3)\n",
    "sm.add('Kdp', -5, 5)\n",
    "sm.add('Expected', 0, 10)\n",
    "sm.add('Prediction', 0, 10)\n",
    "sm.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to look at the correlation between the two types of reflectivity and the prediction. This correlation appears much stronger than it should be, given how strong the correlation is between the reflectivities and the expected values. What it means is that there really isn't much to go on in the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make a plot of the prediction resolution, and check the median of the distribution\n",
    "\n",
    "## Compute resolution\n",
    "resolution = (test['Prediction'] - test['Expected'])/test['Expected']\n",
    "\n",
    "## Compute resolution histogram\n",
    "hist, bins = np.histogram(resolution, bins=51, range=(-10,10))\n",
    "width = (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "## Make the plot\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.xlabel('(Prediction-Expected)/Expected', fontsize=12, color='black')\n",
    "plt.show()\n",
    "print 'Resolution median:', resolution.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the resolution plot isn't peaked at 0 as we would expect, and it has a rather large high tail. The BRT model tends to overestimate the amount of rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The mean absolute error averages the differences between predictions and target. A small shift in the prediction\n",
    "## can improve the resulting mean absolute error, resulting in a better position on the leaderboard. The scientific\n",
    "## justification for this however is dubious at best. But this is the kind of thing the Kaggle format encourages\n",
    "## you to do...\n",
    "\n",
    "## ---------------------------------------------\n",
    "def find_best_prediction_shift(test_df, trainer):\n",
    "    \"\"\"\n",
    "    Finds the best score shift\n",
    "    \"\"\"\n",
    "    min_score  = float('inf')\n",
    "    best_shift = None\n",
    "\n",
    "    for shift in [0.001*i for i in range(1001)]:\n",
    "        test_df['backup'] = test_df['Prediction']\n",
    "        test_df['Prediction'] = test_df['Prediction'] - shift\n",
    "        score = trainer.evaluate(test_df)\n",
    "        if score < min_score:\n",
    "            min_score = score\n",
    "            best_shift = shift\n",
    "        test_df['Prediction'] = test_df['backup']\n",
    "    \n",
    "    return min_score, best_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Find the best shift for the polarized BRT\n",
    "\n",
    "min_score, best_shift = find_best_prediction_shift(test, polarized_trainer)\n",
    "print min_score, best_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Again, make a plot of the prediction resolution, and check the median of the distribution\n",
    "## after shifting by the value that was just found\n",
    "\n",
    "## Apply the shift \n",
    "test['Prediction'] = test['Prediction'] - best_shift\n",
    "\n",
    "## Compute resolution\n",
    "resolution = (test['Prediction'] - test['Expected'])/test['Expected']\n",
    "\n",
    "## Compute resolution histogram\n",
    "hist, bins = np.histogram(resolution, bins=51, range=(-10,10))\n",
    "width = (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "## Make the plot\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.xlabel('(Prediction-Expected)/Expected', fontsize=12, color='black')\n",
    "plt.show()\n",
    "print 'Resolution median:', resolution.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the peak of the distribution is closer to 0, and the median is pretty much at 0 now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's train on the entire training dataset now, and finalize our predictions for the Polarized DF.\n",
    "\n",
    "polarized_trainer = BRTTrainer(\n",
    "    polarized_collapsed_df,\n",
    "    variables=all_columns,\n",
    "    max_depth=3,\n",
    "    n_estimators=200\n",
    "    )\n",
    "\n",
    "polarized_trainer.train(\n",
    "    clean=True,\n",
    "    stuck_repeat=10,\n",
    "    max_expected=20.0\n",
    "    )\n",
    "\n",
    "polarized_trainer.apply(test_polarized_collapsed_df)\n",
    "test_polarized_collapsed_df['Prediction'] = test_polarized_collapsed_df['Prediction'] - best_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Basic DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Split the training dataset in training and testing datasets for crude BRT optimization, but also to check the effect\n",
    "## of doing additional data cleaning on the training dataset on the BRT performance\n",
    "\n",
    "split = len(basic_collapsed_df)/2\n",
    "train = basic_collapsed_df[:split]\n",
    "test  = basic_collapsed_df[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train a BRT on half the training dataset. Parameters have been roughly optimized by hand\n",
    "\n",
    "basic_trainer = BRTTrainer(train, variables=basic_columns, max_depth=3, n_estimators=100)\n",
    "basic_trainer.train(clean=True, stuck_repeat=10, max_expected=30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Evaluate on the second half of the training dataset\n",
    "\n",
    "basic_trainer.apply(test)\n",
    "basic_trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make a scatter plot matrix for the Basic DF\n",
    "sm = ScatterMatrix(test)\n",
    "sm.add('radardist_km', 0, 20)\n",
    "sm.add('Ref', 0, 50)\n",
    "sm.add('RefComposite', 0, 100)\n",
    "sm.add('Expected', 0, 10)\n",
    "sm.add('Prediction', 0, 10)\n",
    "sm.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make a plot of the prediction resolution, and check the median of the distribution\n",
    "\n",
    "## Compute resolution\n",
    "resolution = (test['Prediction'] - test['Expected'])/test['Expected']\n",
    "\n",
    "## Compute resolution histogram\n",
    "hist, bins = np.histogram(resolution, bins=51, range=(-10,10))\n",
    "width = (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "## Make the plot\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.xlabel('(Prediction-Expected)/Expected', fontsize=12, color='black')\n",
    "plt.show()\n",
    "print 'Resolution median:', resolution.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Find the best shift for the basic BRT\n",
    "\n",
    "min_score, best_shift = find_best_prediction_shift(test, basic_trainer)\n",
    "print min_score, best_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Again, make a plot of the prediction resolution, and check the median of the distribution\n",
    "## after shifting by the value that was just found\n",
    "\n",
    "## Apply the shift \n",
    "test['Prediction'] = test['Prediction'] - best_shift\n",
    "\n",
    "## Compute resolution\n",
    "resolution = (test['Prediction'] - test['Expected'])/test['Expected']\n",
    "\n",
    "## Compute resolution histogram\n",
    "hist, bins = np.histogram(resolution, bins=51, range=(-10,10))\n",
    "width = (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "## Make the plot\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.xlabel('(Prediction-Expected)/Expected', fontsize=12, color='black')\n",
    "plt.show()\n",
    "print 'Resolution median:', resolution.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's train on the entire training dataset now, and finalize our predictions for the Basic DF.\n",
    "\n",
    "basic_trainer = BRTTrainer(\n",
    "    basic_collapsed_df,\n",
    "    variables=basic_columns,\n",
    "    max_depth=3,\n",
    "    n_estimators=100\n",
    "    )\n",
    "\n",
    "basic_trainer.train(\n",
    "    clean=True,\n",
    "    stuck_repeat=10,\n",
    "    max_expected=30.0\n",
    "    )\n",
    "\n",
    "basic_trainer.apply(test_basic_collapsed_df)\n",
    "test_basic_collapsed_df['Prediction'] = test_basic_collapsed_df['Prediction'] - best_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Null DF\n",
    "\n",
    "The Null DF has no usable input data, so we'll simply use the median Expected value as the prediction. It doesn't count on the leaderboard anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Split into training and testing, check what kind of accuracy we get with the median as the prediction\n",
    "\n",
    "split = len(null_collapsed_df)/2\n",
    "train = null_collapsed_df[:split]\n",
    "test  = null_collapsed_df[split:]\n",
    "\n",
    "test['Prediction'] = train['Expected'].median()\n",
    "\n",
    "## Use the basic trainer to evaluate, the evaluate method is independent of the BRT anyway\n",
    "basic_trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Finalize the predictions for the Null DF\n",
    "\n",
    "test_null_collapsed_df['Prediction'] = null_collapsed_df['Expected'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Partial Polarized DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Split between training and testing datasets\n",
    "\n",
    "split = len(partial_polarized_collapsed_df)/2\n",
    "train = partial_polarized_collapsed_df[:split]\n",
    "test  = partial_polarized_collapsed_df[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Train the BRT on the training half and evaluate\n",
    "\n",
    "partial_polarized_trainer = BRTTrainer(train, variables=all_columns, max_depth=3, n_estimators=300)\n",
    "partial_polarized_trainer.train(clean=True, stuck_repeat=100, max_expected=20.0)\n",
    "\n",
    "partial_polarized_trainer.apply(test)\n",
    "partial_polarized_trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make a scatter plot matrix for the Polarized DF\n",
    "sm = ScatterMatrix(test)\n",
    "sm.add('radardist_km', 0, 20)\n",
    "sm.add('Ref', 0, 50)\n",
    "sm.add('RefComposite', 0, 50)\n",
    "sm.add('RhoHV', 0.9, 1.05)\n",
    "sm.add('Zdr', -2,3)\n",
    "sm.add('Kdp', -5, 5)\n",
    "sm.add('Expected', 0, 10)\n",
    "sm.add('Prediction', 0, 10)\n",
    "sm.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make a plot of the prediction resolution, and check the median of the distribution\n",
    "\n",
    "## Compute resolution\n",
    "resolution = (test['Prediction'] - test['Expected'])/test['Expected']\n",
    "\n",
    "## Compute resolution histogram\n",
    "hist, bins = np.histogram(resolution, bins=51, range=(-10,10))\n",
    "width = (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "## Make the plot\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.xlabel('(Prediction-Expected)/Expected', fontsize=12, color='black')\n",
    "plt.show()\n",
    "print 'Resolution median:', resolution.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Find the best shift for the polarized BRT\n",
    "\n",
    "min_score, best_shift = find_best_prediction_shift(test, partial_polarized_trainer)\n",
    "print min_score, best_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Again, make a plot of the prediction resolution, and check the median of the distribution\n",
    "## after shifting by the value that was just found\n",
    "\n",
    "## Apply the shift \n",
    "test['Prediction'] = test['Prediction'] - best_shift\n",
    "\n",
    "## Compute resolution\n",
    "resolution = (test['Prediction'] - test['Expected'])/test['Expected']\n",
    "\n",
    "## Compute resolution histogram\n",
    "hist, bins = np.histogram(resolution, bins=51, range=(-10,10))\n",
    "width = (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "## Make the plot\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.xlabel('(Prediction-Expected)/Expected', fontsize=12, color='black')\n",
    "plt.show()\n",
    "print 'Resolution median:', resolution.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's train on the entire training dataset now, and finalize our predictions for the Partial Polarized DF.\n",
    "\n",
    "partial_polarized_trainer = BRTTrainer(\n",
    "    partial_polarized_collapsed_df,\n",
    "    variables=all_columns,\n",
    "    max_depth=3,\n",
    "    n_estimators=300\n",
    "    )\n",
    "\n",
    "partial_polarized_trainer.train(\n",
    "    clean=True,\n",
    "    stuck_repeat=100,\n",
    "    max_expected=20.0\n",
    "    )\n",
    "\n",
    "partial_polarized_trainer.apply(test_partial_polarized_collapsed_df)\n",
    "test_partial_polarized_collapsed_df['Prediction'] = test_partial_polarized_collapsed_df['Prediction'] - best_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Partial Basic DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Split between testing and training datasets, train and evaluate\n",
    "\n",
    "split = len(partial_basic_collapsed_df)/2\n",
    "train = partial_basic_collapsed_df[:split]\n",
    "test  = partial_basic_collapsed_df[split:]\n",
    "\n",
    "partial_basic_trainer = BRTTrainer(train, variables=basic_columns, max_depth=3, n_estimators=300)\n",
    "partial_basic_trainer.train(clean=True, stuck_repeat=50, max_expected=20.0)\n",
    "\n",
    "partial_basic_trainer.apply(test)\n",
    "partial_basic_trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make a scatter plot matrix for the Partial Basic DF\n",
    "sm = ScatterMatrix(test)\n",
    "sm.add('radardist_km', 0, 20)\n",
    "sm.add('Ref', 0, 50)\n",
    "sm.add('RefComposite', 0, 100)\n",
    "sm.add('Expected', 0, 10)\n",
    "sm.add('Prediction', 0, 10)\n",
    "sm.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make a plot of the prediction resolution, and check the median of the distribution\n",
    "\n",
    "## Compute resolution\n",
    "resolution = (test['Prediction'] - test['Expected'])/test['Expected']\n",
    "\n",
    "## Compute resolution histogram\n",
    "hist, bins = np.histogram(resolution, bins=51, range=(-10,10))\n",
    "width = (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "## Make the plot\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.xlabel('(Prediction-Expected)/Expected', fontsize=12, color='black')\n",
    "plt.show()\n",
    "print 'Resolution median:', resolution.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Find the best shift for the polarized BRT\n",
    "\n",
    "min_score, best_shift = find_best_prediction_shift(test, partial_basic_trainer)\n",
    "print min_score, best_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Again, make a plot of the prediction resolution, and check the median of the distribution\n",
    "## after shifting by the value that was just found\n",
    "\n",
    "## Apply the shift \n",
    "test['Prediction'] = test['Prediction'] - best_shift\n",
    "\n",
    "## Compute resolution\n",
    "resolution = (test['Prediction'] - test['Expected'])/test['Expected']\n",
    "\n",
    "## Compute resolution histogram\n",
    "hist, bins = np.histogram(resolution, bins=51, range=(-10,10))\n",
    "width = (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "## Make the plot\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.xlabel('(Prediction-Expected)/Expected', fontsize=12, color='black')\n",
    "plt.show()\n",
    "print 'Resolution median:', resolution.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's train on the entire training dataset now, and finalize our predictions for the Partial Polarized DF.\n",
    "\n",
    "partial_basic_trainer = BRTTrainer(\n",
    "    partial_basic_collapsed_df,\n",
    "    variables=basic_columns,\n",
    "    max_depth=3,\n",
    "    n_estimators=300\n",
    "    )\n",
    "\n",
    "partial_basic_trainer.train(\n",
    "    clean=True,\n",
    "    stuck_repeat=50,\n",
    "    max_expected=20.0\n",
    "    )\n",
    "\n",
    "partial_basic_trainer.apply(test_partial_basic_collapsed_df)\n",
    "test_partial_basic_collapsed_df['Prediction'] = test_partial_basic_collapsed_df['Prediction'] - best_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Concatenate the categories back into a single DF\n",
    "\n",
    "test_dfs = [\n",
    "    test_polarized_collapsed_df,\n",
    "    test_basic_collapsed_df,\n",
    "    test_null_collapsed_df,\n",
    "    test_partial_polarized_collapsed_df,\n",
    "    test_partial_basic_collapsed_df\n",
    "]\n",
    "\n",
    "test_with_predictions = pd.concat(test_dfs)\n",
    "\n",
    "## Check that nothing was lost, expecting 717625 predictions\n",
    "print 'Number of predictions:', len(test_with_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Produce the .csv predictions file\n",
    "test_with_predictions.sort_values(by=['Id'], inplace=True)\n",
    "test_with_predictions.rename(columns={'Prediction' : 'Expected'}, inplace=True)\n",
    "test_with_predictions[['Id', 'Expected']].to_csv('predictions_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
